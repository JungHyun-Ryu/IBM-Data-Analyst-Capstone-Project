{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "prev_pub_hash": "fee3cd0da99af27f7f7a8d9c340e8e78f253ffc32a2251cadcb6968f21d952d5"
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "<p style=\"text-align:center\">\n    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDA0321ENSkillsNetwork928-2022-01-01\" target=\"_blank\">\n    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"400\" alt=\"Skills Network Logo\"  />\n    </a>\n</p>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# **Data Wrangling Lab**\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Estimated time needed: **45 to 60** minutes\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "In this assignment you will be performing data wrangling.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Objectives\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "In this lab you will perform the following:\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "-   Identify duplicate values in the dataset.\n\n-   Remove duplicate values from the dataset.\n\n-   Identify missing values in the dataset.\n\n-   Impute the missing values in the dataset.\n\n-   Normalize data in the dataset.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<hr>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Hands on Lab\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Import pandas module.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "<ipython-input-1-7dd3504c366f>:1: DeprecationWarning: \nPyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\nbut was not found to be installed on your system.\nIf this would cause problems for you,\nplease provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n        \n  import pandas as pd\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": "Load the dataset into a dataframe.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<h2>Read Data</h2>\n<p>\nWe utilize the <code>pandas.read_csv()</code> function for reading CSV files. However, in this version of the lab, which operates on JupyterLite, the dataset needs to be downloaded to the interface using the provided code below.\n</p>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "The functions below will download the dataset into your browser:\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from pyodide.http import pyfetch\n\nasync def download(url, filename):\n    response = await pyfetch(url)\n    if response.status == 200:\n        with open(filename, \"wb\") as f:\n            f.write(await response.bytes())",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": "file_path = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DA0321EN-SkillsNetwork/LargeData/m1_survey_data.csv\"",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": "To obtain the dataset, utilize the download() function as defined above:  \n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "await download(file_path, \"m1_survey_data.csv\")\nfile_name=\"m1_survey_data.csv\"",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "source": "Utilize the Pandas method read_csv() to load the data into a dataframe.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df = pd.read_csv(file_name)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "markdown",
      "source": "> Note: This version of the lab is working on JupyterLite, which requires the dataset to be downloaded to the interface.While working on the downloaded version of this notebook on their local machines(Jupyter Anaconda), the learners can simply **skip the steps above,** and simply use the URL directly in the `pandas.read_csv()` function. You can uncomment and run the statements in the cell below.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#df = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DA0321EN-SkillsNetwork/LargeData/m1_survey_data.csv\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "source": "## Finding duplicates\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "In this section you will identify duplicate values in the dataset.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": " Find how many duplicate rows exist in the dataframe.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# your code goes here\n\nduplicate_count = df.duplicated().sum()\n\n# Display the first few duplicate rows to understand their structure\nduplicate_rows = df[df.duplicated()]\n\nprint(f\"Number of duplicate rows: {duplicate_count}\")\nprint(\"\\nFirst few duplicate rows:\")\nprint(duplicate_rows.head())",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Number of duplicate rows: 154\n\nFirst few duplicate rows:\n      Respondent                      MainBranch Hobbyist  \\\n1168        2339  I am a developer by profession      Yes   \n1169        2342  I am a developer by profession      Yes   \n1170        2343  I am a developer by profession      Yes   \n1171        2344  I am a developer by profession      Yes   \n1172        2347  I am a developer by profession      Yes   \n\n                                            OpenSourcer  \\\n1168                         Once a month or more often   \n1169                                              Never   \n1170  Less than once a month but more than once per ...   \n1171                                              Never   \n1172                                              Never   \n\n                                             OpenSource          Employment  \\\n1168  OSS is, on average, of HIGHER quality than pro...  Employed full-time   \n1169  The quality of OSS and closed source software ...  Employed full-time   \n1170  OSS is, on average, of LOWER quality than prop...  Employed full-time   \n1171  The quality of OSS and closed source software ...  Employed full-time   \n1172  OSS is, on average, of HIGHER quality than pro...  Employed full-time   \n\n             Country Student  \\\n1168   United States      No   \n1169  United Kingdom      No   \n1170          Canada      No   \n1171   United States      No   \n1172  United Kingdom      No   \n\n                                                EdLevel  \\\n1168  Some college/university study without earning ...   \n1169  Some college/university study without earning ...   \n1170        Master’s degree (MA, MS, M.Eng., MBA, etc.)   \n1171           Bachelor’s degree (BA, BS, B.Eng., etc.)   \n1172        Master’s degree (MA, MS, M.Eng., MBA, etc.)   \n\n                                         UndergradMajor  ...  \\\n1168  Computer science, computer engineering, or sof...  ...   \n1169  Information systems, information technology, o...  ...   \n1170  Computer science, computer engineering, or sof...  ...   \n1171  Computer science, computer engineering, or sof...  ...   \n1172  Computer science, computer engineering, or sof...  ...   \n\n                                 WelcomeChange  \\\n1168   Just as welcome now as I felt last year   \n1169  Somewhat more welcome now than last year   \n1170  Somewhat more welcome now than last year   \n1171   Just as welcome now as I felt last year   \n1172   Just as welcome now as I felt last year   \n\n                                           SONewContent   Age Gender Trans  \\\n1168                                                NaN  24.0    Man    No   \n1169  Tech meetups or events in your area;Courses on...  24.0    Man    No   \n1170  Tech articles written by other developers;Indu...  27.0    Man    No   \n1171  Tech articles written by other developers;Indu...  24.0    Man    No   \n1172                                                NaN   NaN  Woman    No   \n\n                    Sexuality  \\\n1168  Straight / Heterosexual   \n1169  Straight / Heterosexual   \n1170  Straight / Heterosexual   \n1171  Straight / Heterosexual   \n1172  Straight / Heterosexual   \n\n                                              Ethnicity Dependents  \\\n1168                       White or of European descent         No   \n1169                       White or of European descent         No   \n1170  Black or of African descent;White or of Europe...         No   \n1171                       White or of European descent         No   \n1172                                           Biracial         No   \n\n               SurveyLength                  SurveyEase  \n1168  Appropriate in length                        Easy  \n1169               Too long                        Easy  \n1170  Appropriate in length  Neither easy nor difficult  \n1171  Appropriate in length                        Easy  \n1172               Too long                        Easy  \n\n[5 rows x 85 columns]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 10
    },
    {
      "cell_type": "code",
      "source": "# additional code for Graded Quiz \n# How many duplicate values are there in the column Respondent? \n\nduplicate_count = df['Respondent'].duplicated().sum()\n\nprint(f\"Number of duplicate values in the 'Respondent' column: {duplicate_count}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Number of duplicate values in the 'Respondent' column: 154\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 8
    },
    {
      "cell_type": "markdown",
      "source": "## Removing duplicates\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Remove the duplicate rows from the dataframe.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# your code goes here\n\n# Remove duplicates\ndf_unique = df.drop_duplicates(subset = duplicate_rows)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 11
    },
    {
      "cell_type": "markdown",
      "source": "Verify if duplicates were actually dropped.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Calculate the number of rows before and after removing duplicates\nrows_before = df.shape[0]\nrows_after = df_unique.shape[0]\n\n# Calculate the number of removed duplicates\nremoved_duplicates = rows_before - rows_after\n\n# Print the results\nprint(f\"Number of rows before removing duplicates: {rows_before}\")\nprint(f\"Number of rows after removing duplicates: {rows_after}\")\nprint(f\"Number of removed duplicates: {removed_duplicates}\")\nprint(f\"Number of duplicate rows: {duplicate_count}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Number of rows before removing duplicates: 11552\nNumber of rows after removing duplicates: 11398\nNumber of removed duplicates: 154\nNumber of duplicate rows: 154\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 12
    },
    {
      "cell_type": "code",
      "source": "# additional code for graded quiz\n# After removing the duplicate rows, how many blank rows are there under the column EdLevel?\n\n# Count blank rows in the 'EdLevel' column\nblank_edlevel_count = df_unique['EdLevel'].isna().sum()\n\nprint(f\"Number of blank rows in the 'EdLevel' column after removing duplicates: {blank_edlevel_count}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Number of blank rows in the 'EdLevel' column after removing duplicates: 112\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 13
    },
    {
      "cell_type": "markdown",
      "source": "## Finding Missing values\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Find the missing values for all columns.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# your code goes here\n\n# Set option to display all rows\npd.set_option('display.max_rows', None)\n\n# Your existing code to calculate missing values\nmissing_values = df.isnull().sum()\n\n# Display the missing values for all columns\nprint(\"Missing values for all columns:\")\nprint(missing_values)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Missing values for all columns:\nRespondent                   0\nMainBranch                   0\nHobbyist                     0\nOpenSourcer                  0\nOpenSource                  81\nEmployment                   0\nCountry                      0\nStudent                     53\nEdLevel                    116\nUndergradMajor             740\nEduOther                   164\nOrgSize                     98\nDevType                     67\nYearsCode                    9\nAge1stCode                  13\nYearsCodePro                16\nCareerSat                    0\nJobSat                       1\nMgrIdiot                   498\nMgrMoney                   502\nMgrWant                    498\nJobSeek                      0\nLastHireDate                 0\nLastInt                    423\nFizzBuzz                    37\nJobFactors                   3\nResumeUpdate                41\nCurrencySymbol               0\nCurrencyDesc                 0\nCompTotal                  815\nCompFreq                   206\nConvertedComp              822\nWorkWeekHrs                125\nWorkPlan                   123\nWorkChallenge              168\nWorkRemote                   8\nWorkLoc                     32\nImpSyn                       5\nCodeRev                      1\nCodeRevHrs                2469\nUnitTests                   29\nPurchaseHow                198\nPurchaseWhat                38\nLanguageWorkedWith          11\nLanguageDesireNextYear     137\nDatabaseWorkedWith         456\nDatabaseDesireNextYear    1055\nPlatformWorkedWith         422\nPlatformDesireNextYear     561\nWebFrameWorkedWith        1413\nWebFrameDesireNextYear    1634\nMiscTechWorkedWith        2209\nMiscTechDesireNextYear    1474\nDevEnviron                  29\nOpSys                       34\nContainers                  82\nBlockchainOrg             2354\nBlockchainIs              2637\nBetterLife                 100\nITperson                    35\nOffOn                       38\nSocialMedia                301\nExtraversion                20\nScreenName                 513\nSOVisit1st                 325\nSOVisitFreq                  5\nSOVisitTo                    1\nSOFindAnswer                 3\nSOTimeSaved                 51\nSOHowMuchTime             1936\nSOAccount                    1\nSOPartFreq                1148\nSOJobs                       6\nEntTeams                     5\nSOComm                       0\nWelcomeChange               89\nSONewContent              1995\nAge                        297\nGender                      75\nTrans                      123\nSexuality                  547\nEthnicity                  683\nDependents                 144\nSurveyLength                19\nSurveyEase                  14\ndtype: int64\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 14
    },
    {
      "cell_type": "markdown",
      "source": "Find out how many rows are missing in the column 'WorkLoc'\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# your code goes here\n\nWorkLoc_missing_values = df['WorkLoc'].isnull().sum()\nprint(WorkLoc_missing_values)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "32\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 25
    },
    {
      "cell_type": "markdown",
      "source": "## Imputing missing values\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Find the  value counts for the column WorkLoc.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# your code goes here\n\n# Find value counts for the WorkLoc column\nwork_loc_counts = df['WorkLoc'].value_counts()\n\n# Display the results\nprint(\"Value counts for WorkLoc column:\")\nprint(work_loc_counts)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Value counts for WorkLoc column:\nWorkLoc\nOffice                                            6905\nHome                                              3638\nOther place, such as a coworking space or cafe     977\nName: count, dtype: int64\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 28
    },
    {
      "cell_type": "code",
      "source": "# Optional: Display as percentage\n\n# Calculate percentages\nwork_loc_percentages = (work_loc_counts / len(df) * 100).round(2)\n\n# Display the results\nprint(\"Value counts for WorkLoc column:\")\nprint(work_loc_counts)\n\nprint(\"\\nValue counts as percentage:\")\nfor location, percentage in work_loc_percentages.items():\n    print(f\"{location}: {percentage:.2f}%\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Value counts for WorkLoc column:\nWorkLoc\nOffice                                            6905\nHome                                              3638\nOther place, such as a coworking space or cafe     977\nName: count, dtype: int64\n\nValue counts as percentage:\nOffice: 59.77%\nHome: 31.49%\nOther place, such as a coworking space or cafe: 8.46%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 34
    },
    {
      "cell_type": "markdown",
      "source": "Identify the value that is most frequent (majority) in the WorkLoc column.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#make a note of the majority value here, for future reference\n\nmost_frequent_value = df['WorkLoc'].mode()[0]\nprint(\"\\nMost frequent value in 'WorkLoc' column:\")\nprint(most_frequent_value)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\nMost frequent value in 'WorkLoc' column:\nOffice\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 30
    },
    {
      "cell_type": "markdown",
      "source": "Impute (replace) all the empty rows in the column WorkLoc with the value that you have identified as majority.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Impute the missing values in the 'WorkLoc' column with the most frequent value\ndf['WorkLoc'] = df['WorkLoc'].fillna(most_frequent_value)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 39
    },
    {
      "cell_type": "markdown",
      "source": "After imputation there should ideally not be any empty rows in the WorkLoc column.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Verify if imputing was successful.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# your code goes here\n\n# Verify that there are no remaining missing values in the 'WorkLoc'' column after imputation\nremaining_missing_values = df['WorkLoc'].isnull().sum()\n\nprint(\"\\nMost frequent value in 'WorkLoc'' column:\")\nprint(most_frequent_value)\n\nprint(\"\\nRemaining missing values in 'WorkLoc'' column after imputation:\")\nprint(remaining_missing_values)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\nMost frequent value in 'WorkLoc'' column:\nOffice\n\nRemaining missing values in 'WorkLoc'' column after imputation:\n0\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 40
    },
    {
      "cell_type": "code",
      "source": "# additional code for graded quiz\n# What is the majority category under the column Employment?\n\nmost_frequent_value2 = df['Employment'].mode()[0]\nprint(most_frequent_value2)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Employed full-time\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 15
    },
    {
      "cell_type": "code",
      "source": "# additional code for graded quiz\n# Under the column \" UndergradMajor\", which category has the minimum number of rows?\n\n# Find value counts for the WorkLoc column\nUndergradMajor_counts = df['UndergradMajor'].value_counts()\n\n# Calculate percentages\nUndergradMajor_percentages = (UndergradMajor_counts / len(df) * 100).round(2)\n\n# Display the results\nprint(\"Value counts for UndergradMajor column:\")\nprint(UndergradMajor_counts)\n\nprint(\"\\nValue counts as percentage:\")\nfor major, percentage in UndergradMajor_percentages.items():\n    print(f\"{major}: {percentage:.2f}%\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Value counts for UndergradMajor column:\nUndergradMajor\nComputer science, computer engineering, or software engineering          7053\nInformation systems, information technology, or system administration     806\nAnother engineering discipline (ex. civil, electrical, mechanical)        769\nWeb development or web design                                             417\nA natural science (ex. biology, chemistry, physics)                       409\nMathematics or statistics                                                 375\nA business discipline (ex. accounting, finance, marketing)                247\nA social science (ex. anthropology, psychology, political science)        212\nA humanities discipline (ex. literature, history, philosophy)             209\nFine arts or performing arts (ex. graphic design, music, studio art)      165\nI never declared a major                                                  126\nA health science (ex. nursing, pharmacy, radiology)                        24\nName: count, dtype: int64\n\nValue counts as percentage:\nComputer science, computer engineering, or software engineering: 61.05%\nInformation systems, information technology, or system administration: 6.98%\nAnother engineering discipline (ex. civil, electrical, mechanical): 6.66%\nWeb development or web design: 3.61%\nA natural science (ex. biology, chemistry, physics): 3.54%\nMathematics or statistics: 3.25%\nA business discipline (ex. accounting, finance, marketing): 2.14%\nA social science (ex. anthropology, psychology, political science): 1.84%\nA humanities discipline (ex. literature, history, philosophy): 1.81%\nFine arts or performing arts (ex. graphic design, music, studio art): 1.43%\nI never declared a major: 1.09%\nA health science (ex. nursing, pharmacy, radiology): 0.21%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 16
    },
    {
      "cell_type": "code",
      "source": "# additional code for graded quiz\n# After removing the duplicate rows, how many respondents are being paid yearly?\n\n# Remove duplicate rows\ndf_unique = df.drop_duplicates(subset=['Respondent'])\n\n# Count respondents paid yearly\nyearly_count = df_unique[df_unique['CompFreq'] == 'Yearly'].shape[0]\n\nprint(f\"Number of respondents being paid yearly: {yearly_count}\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Number of respondents being paid yearly: 6073\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 21
    },
    {
      "cell_type": "markdown",
      "source": "## Normalizing data\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "There are two columns in the dataset that talk about compensation.\n\nOne is \"CompFreq\". This column shows how often a developer is paid (Yearly, Monthly, Weekly).\n\nThe other is \"CompTotal\". This column talks about how much the developer is paid per Year, Month, or Week depending upon his/her \"CompFreq\". \n\nThis makes it difficult to compare the total compensation of the developers.\n\nIn this section you will create a new column called 'NormalizedAnnualCompensation' which contains the 'Annual Compensation' irrespective of the 'CompFreq'.\n\nOnce this column is ready, it makes comparison of salaries easy.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<hr>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "List out the various categories in the column 'CompFreq'\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# your code goes here\n\ncomp_freq_categories = df['CompFreq'].unique()\nprint(\"Categories in CompFreq column:\")\nprint(comp_freq_categories)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Categories in CompFreq column:\n['Yearly' 'Monthly' 'Weekly' nan]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 18
    },
    {
      "cell_type": "markdown",
      "source": "Create a new column named 'NormalizedAnnualCompensation'. Use the hint given below if needed.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Double click to see the **Hint**.\n\n<!--\n\nUse the below logic to arrive at the values for the column NormalizedAnnualCompensation.\n\nIf the CompFreq is Yearly then use the exising value in CompTotal\nIf the CompFreq is Monthly then multiply the value in CompTotal with 12 (months in an year)\nIf the CompFreq is Weekly then multiply the value in CompTotal with 52 (weeks in an year)\n\n-->\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# your code goes here\n\nimport numpy as np\n\ndef normalize_annual_compensation(row):\n    if pd.isna(row['CompFreq']) or pd.isna(row['CompTotal']):\n        return np.nan\n    elif row['CompFreq'] == 'Yearly':\n        return row['CompTotal']\n    elif row['CompFreq'] == 'Monthly':\n        return row['CompTotal'] * 12\n    elif row['CompFreq'] == 'Weekly':\n        return row['CompTotal'] * 52\n    else:\n        return np.nan\n\n# Create the new column\ndf['NormalizedAnnualCompensation'] = df.apply(normalize_annual_compensation, axis=1)\n\n# Display the first few rows to verify the result\nprint(df[['CompFreq', 'CompTotal', 'NormalizedAnnualCompensation']].head(10))\n\n# Display summary statistics\nprint(\"\\nSummary statistics of NormalizedAnnualCompensation:\")\nprint(df['NormalizedAnnualCompensation'].describe())",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "  CompFreq  CompTotal  NormalizedAnnualCompensation\n0   Yearly    61000.0                       61000.0\n1   Yearly   138000.0                      138000.0\n2   Yearly    90000.0                       90000.0\n3  Monthly    29000.0                      348000.0\n4   Yearly    90000.0                       90000.0\n5  Monthly     9500.0                      114000.0\n6  Monthly     3000.0                       36000.0\n7   Yearly   103000.0                      103000.0\n8   Yearly    69000.0                       69000.0\n9  Monthly     8000.0                       96000.0\n\nSummary statistics of NormalizedAnnualCompensation:\ncount    1.073100e+04\nmean     6.076494e+06\nstd      9.774169e+07\nmin      0.000000e+00\n25%      5.200000e+04\n50%      1.000000e+05\n75%      3.600000e+05\nmax      8.400000e+09\nName: NormalizedAnnualCompensation, dtype: float64\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 19
    },
    {
      "cell_type": "code",
      "source": "df['NormalizedAnnualCompensation'].median()",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "execution_count": 20,
          "output_type": "execute_result",
          "data": {
            "text/plain": "100000.0"
          },
          "metadata": {}
        }
      ],
      "execution_count": 20
    },
    {
      "cell_type": "markdown",
      "source": "## Authors\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Ramesh Sannareddy\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Other Contributors\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Rav Ahuja\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": " ## Change Log\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n|-|-|-|-|\n|2024-09-24|1.1|Madhusudhan Moole|Updated lab|\n|2024-09-23|1.0|Raghul Ramesh|Created lab|\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<!--| Date (YYYY-MM-DD) | Version | Changed By        | Change Description                 |\n| ----------------- | ------- | ----------------- | ---------------------------------- |\n| 2020-10-17        | 0.1     | Ramesh Sannareddy | Created initial version of the lab |--!>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## <h3 align=\"center\"> © IBM Corporation. All rights reserved. <h3/>\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    }
  ]
}